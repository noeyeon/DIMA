


import requests

URL = 'https://www.naver.com'
response = requests.get(URL) #get 방식으로 요청
print(response.status_code)


print(response.text) #네이버 홈페이지에 해당하는 html


URL = 'https://search.naver.com/search.naver?query=python'
query = {'query':'python'}
response = requests.get(URL, params=query)
print(response.status_code)
print(response.text)





'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'


import requests

URL = 'http://www.google.com/search'
params = {'q':'python'} #구글에서는 맵핑을 할 때 q라는 키워드에다 저장을 한다
headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'} #로봇이 아님을 나타내는 user-agent값을 헤더에 넣어 보낸다
response = requests.get(URL, params=params, headers=headers)
response.raise_for_status() #응답코드가 200이 아니면 오류내고 멈춰라
result = response.text

with open('mygoogle.html','w', encoding='utf-8')as f: #f라는 이름에 파일 객체를 담겠다
    f.write(result)
print('저장완료!')


import requests

URL = 'http://www.google.com/search'
params = {'q':'python'}
headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'}
response = requests.get(URL, params=params) #headers=headers) #유저 에이전트 값이 없는 경우
response.raise_for_status() #응답코드가 200이 아니면 오류내고 멈춰리
result = response.text

with open('mygoogle.html','w', encoding='utf-8')as f:
    f.write(result)
print('저장완료!')





#페이지에서  f12를 누르고 개발자 모드의 좌측상당의 화살표 select모드 선택 후 추출하고 싶은 단어를 클릭하면 단어의 위치를 알려준다

#추출하고 싶은 정보에 바로 접근하는 건 힘들고 추출하고자 하는 정보의 상위/형제 태그를 이용해 하위/같은 레벨의 태그에 접근한다

import requests

response = requests.get('https://datalab.naver.com') #요청을 해야하는 웹 사이트의 주소 입력
html_text = response.text #넘어온 응답 메세지를 html_text에 담는다
#근접한/같은 레벨의 em이라는 class로 split(분리)한다.
temp = html_text.split('<em class="num">1</em>')[1]
#em태그의 클래스는 num이라는 값을 가지고 있다.
temp = temp.split('<span class="title">')[1]
#바로 위에서 만들어진 템프를 다시 <span>태그로 split, 속성들이 대부분 ""로 묶여 있기 때문에 ''로 묶어준다.
temp = temp.split('</span>')[0] #다시 </span>태그로 스플릿
print(temp)








#!pip install html5lib
!pip install lxml


from bs4 import BeautifulSoup
soup = BeautifulSoup('<a></p>','html.parser')
print('html.parcer')
print(soup)
print('-'*40)

soup = BeautifulSoup('<a></p>','lxml')
print('lxml')
print(soup)
print('-'*40)

soup = BeautifulSoup('<a></p>','xml')
print('xml')
print(soup)
print('-'*40)

soup = BeautifulSoup('<a></p>','html5lib')
print('html5lib')
print(soup)
print('-'*40)





import requests
from bs4 import BeautifulSoup
#위키피디아 한
URL = 'https://ko.wikipedia.org/wiki/%EC%9B%B9%ED%81%AC%EB%A1%A4%EB%9F%AC' #마지막 부분은 유니코트도 표현된 것 -웹크롤러

response = requests.get(URL)

#soup 객체를 생성하기
soup = BeautifulSoup(response.text, 'lxml')

#태그를 이용한 접근
print(soup.title)
print(soup.footer.ul.li)

#태그는 지우고 안의 값만 추출하기
print(soup.footer.ul.li.text)

#태그와 속성을 이용한 접근
print(soup.a) #soup 객체에서 첫번째로 만나는 a태그를 출력

#print(soup.a['id']) #만약 속성이 존재하지 않으면 에러 발생

print(soup.a['href'])

#find 함수를 이용한 태그 내의 다양한 속성을 이용한 접근
print(soup.find('a',attrs={'title':'구글봇'})) #a태그 중 title 속성의 값이 '구글봇'인 데이터를 검색
